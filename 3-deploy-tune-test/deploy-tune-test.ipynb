{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### Deploy the Model\n",
    "\n",
    "#### 0. Setup the imports, bucket and model reference\n",
    "#### 1. Setup the path to the model\n",
    "#### 2. Deploy the model\n",
    "#### 3. Setup and Prepare the data for testing\n",
    "#### 4. Execute endpoint with the test data\n",
    "#### 5. Cost Analysis - Evaluate the model based on cost\n",
    "#### 6. Evaluate standard metrics used for **Binary Classification**\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point the model artefacts MUST be available in an S3 bucket\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker.amazon.common as smac\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sagemaker Session\n",
    "sess = sagemaker.session.Session()\n",
    "\n",
    "# Setup the execution role - IGNORE THE WARNING\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### 1. Setup the path to the Model\n",
    "    \n",
    "#### Use the Best Model identified in the last Lab\n",
    "    \n",
    "1. <code style=\"background:yellow;color:black\">MUST</code> Setup the bucket name\n",
    "2. You will now use the BEST model's job name (copied to temp file)\n",
    "    * <code style=\"background:yellow;color:black\">MUST </code> set it in variable *specific_model**\n",
    "    * To check out the results from different runs you may use the console\n",
    "    * You will also find the model output folder and name\n",
    "    \n",
    "**Specifying an incorrect path | model will lead to an Error!!**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE the bucket name\n",
    "bucket = 'awsrajeev'\n",
    "\n",
    "# Confirm this\n",
    "prefix = \"sagemaker/churn-analysis/output\"\n",
    "\n",
    "# MUST CHANGE THIS - Model Artefacts folder \n",
    "specific_model=\"xgboost-my-training-job-101-2020-05-14-10-42-07-285\"\n",
    "\n",
    "s3_model_artefacts=\"s3://{}/{}/{}/output/model.tar.gz\".format(bucket, prefix,specific_model)\n",
    "s3_model_artefacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### 2. Deploy the Model as endpoint\n",
    "* The endpoint is launched in a container\n",
    "* Endpoint exposes a REST API that responds with CSV/JSON depending on algo and setup\n",
    "* Implemeneted in a Docker container and hosted on EC2\n",
    "\n",
    "<code style=\"background:yellow;color:black\">Endpoint setup takes a few minutes, so go for a quick break!!!</code>\n",
    "    \n",
    "<code style=\"background:red;color:white\">Endpoint name MUST be unique - so if you try to re install it again it will fail!!</code>\n",
    "    \n",
    "#### Re-Installing the endpoint\n",
    "* Go to SageMaker console and delete the endpoint\n",
    "* Setup the paths to new model in this notebook and run the cells\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the name of the endpoint\n",
    "endpoint_name=\"churn-endpoint\"\n",
    "\n",
    "# Get the model container\n",
    "container = get_image_uri(boto3.Session().region_name,\n",
    "                          'xgboost', \n",
    "                          repo_version='1.0-1'); \n",
    "\n",
    "model = sagemaker.model.Model(model_data=s3_model_artefacts, image=container, role=role, sagemaker_session=sess )\n",
    "model.deploy(initial_instance_count=1,instance_type=\"ml.c4.2xlarge\", endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### 3. Setup and Prepare the data for testing\n",
    "    \n",
    "#### Setup the Data\n",
    "    \n",
    "* The test data was generated as part of the data prep. For convenience its available in the local folder\n",
    "* The test data we will use is in th elocal file  *test_header.csv*\n",
    "    * The first column is the label column - it is the actual value *(1 = Churned, 0 = Not Churned)*\n",
    "    * Check out the *One Hot Encoded* State_ Columns !!!\n",
    "    * In all 70 columns\n",
    "* We will read the data into memory (data frame)\n",
    "* Open the test_header.csv to see the raw data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_header.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "#### Prepare the data for testing\n",
    "    \n",
    "1. Move the label to the end and name it \"Actual\"\n",
    "2. Create a new column in the dataframe and call it \"Predicted\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Move the label column\n",
    "df_churn = df['Churn?_True.']\n",
    "df = pd.concat( [df.drop(['Churn?_True.'], axis=1), df_churn], axis=1)\n",
    "\n",
    "#2. Create a new column at the end - call it predicted\n",
    "count = len(df) \n",
    "df_predicted = pd.Series(-1.0,range(0,count - 1),name=\"Predicted\")\n",
    "df = pd.concat( [df, df_predicted], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "###  4. Execute endpoint with the test data\n",
    "\n",
    "Execute prediction using [API](https://sagemaker.readthedocs.io/en/stable/predictors.html)\n",
    "    \n",
    "1. Setup the predictor object\n",
    "2. In a loop run the test using the data from the file\n",
    "    * Add the predicted value under the column 'Predicted'\n",
    "3. Add the received prediction in the test data frame\n",
    "    \n",
    "<br>\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Used in the analysis\n",
    "test_features = np.array(df.drop(['Churn?_True.',\"Predicted\"], axis=1).values).astype('float32')\n",
    "\n",
    "\n",
    "# At this time you endpoint MUST be up\n",
    "predictor = sagemaker.predictor.RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess)\n",
    "\n",
    "predictor.content_type = 'text/csv'\n",
    "predictor.serializer = csv_serializer\n",
    "predictor.deserializer = json_deserializer\n",
    "predictor.accept = 'text/csv'\n",
    "\n",
    "for i in range(len(df)) :\n",
    "    predicted_churn = predictor.predict(test_features[i])\n",
    "    \n",
    "    # Set the predicted value - probability of churn\n",
    "    df.loc[i, 'Predicted']=predicted_churn\n",
    "\n",
    "# df holds the results from test runcheck out the last 2 columns\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### 5. Cost Analysis - Evaluate the model based on cost\n",
    "Business has to pay a price for the *FP* and *FN*. E.g., if a customer who is going to churn is incorrectly predicted as NOT churn then company will lose the customer!! Here is a simple analysis.    \n",
    "    \n",
    "1. If Customer is NOT going to churn = TN = cost to Atizen = USD 0<br>\n",
    "2. If Customer is LOST then Aitzen will lose = FN = USD 1000 per annum<br>\n",
    "3. If Customer is about to Churn BUT Atizen offered Promotion = TP = USD 100 per annum<br>\n",
    "4. If Customer is NOT going to Churn BUT Atizen offered Promotion = FP = USD 1000 per annum\n",
    "    \n",
    "    * <code style=\"background:yellow;color:black\">Cost Incurred by Aitzen</code>= TN * 0 + FN * 1000 + TP * 100 + FP * 1000\n",
    "\n",
    "<br>\n",
    "    \n",
    "#### Churn Factor\n",
    "Predictor return a probabilty not an absolute 0 or 1. You need to decide whether predicted value = 0.5 represents a customer that is going to churn or NOT!!! Sensitivity of the model is controlled by adjusting this factor. No hard and fast rule but try out multiple values to <code style=\"background:yellow;color:black\">MINIMIZE the cost</code>.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates the cost based on churn_factor\n",
    "def calculate_confusion_matrix_and_cost(churn_factor):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    # ADJUST this for making changes for cost\n",
    "    # FN & FP depends on this custoff\n",
    "#     churn_cutoff = 0.8\n",
    "\n",
    "    for i in range(len(df)) :\n",
    "\n",
    "        predicted_churn = df.iloc[i]['Predicted']\n",
    "\n",
    "        if (predicted_churn > churn_factor):\n",
    "            predicted_churn=1\n",
    "        else:\n",
    "            predicted_churn=0\n",
    "\n",
    "        # Set the values of TN & FP\n",
    "        if (df.iloc[i]['Churn?_True.'] == 0.0) :\n",
    "            if (predicted_churn == 0.0) :\n",
    "                # Actual -ve    Predicted  -ve\n",
    "                TN += 1\n",
    "            else:\n",
    "                # Actual -ve    Predicted  +ve\n",
    "                 FP += 1\n",
    "\n",
    "        # Set the values of TP and FN\n",
    "        if (df.iloc[i][\"Churn?_True.\"] == 1.0):\n",
    "            if (predicted_churn == 1.0 ) :\n",
    "                # Actual +ve    Predicted  +ve\n",
    "                TP += 1\n",
    "            else:\n",
    "                # Actual +ve    Predicted  -ve\n",
    "                FN += 1\n",
    "\n",
    "    # Setup the confusion matrix\n",
    "#     confusion_matrix = pd.DataFrame(index =['predicted_true', 'predicted_false']) \n",
    "#     confusion_matrix.loc['predicted_true', 'actual_true'] = 'TP='+str(TP)\n",
    "#     confusion_matrix.loc['predicted_true', 'actual_false'] = 'FP='+str(FP)\n",
    "#     confusion_matrix.loc['predicted_false', 'actual_true'] = 'FN='+str(FN)\n",
    "#     confusion_matrix.loc['predicted_false', 'actual_false'] = 'TN='+str(TN)\n",
    "    \n",
    "    confusion_matrix = pd.DataFrame(index =['predicted_true', 'predicted_false']) \n",
    "    confusion_matrix.loc['predicted_true', 'actual_true'] = TP\n",
    "    confusion_matrix.loc['predicted_true', 'actual_false'] = FP\n",
    "    confusion_matrix.loc['predicted_false', 'actual_true'] = FN\n",
    "    confusion_matrix.loc['predicted_false', 'actual_false'] = TN\n",
    "#     display(confusion_matrix)\n",
    "\n",
    "    # Cost \n",
    "    cost =  TN * 0 + FN * 1000 + TP * 100 + FP * 1000\n",
    "#     display(\"Cost = $\"+str(cost))\n",
    "    \n",
    "    return cost, confusion_matrix\n",
    "    \n",
    "# Execute with churn factor = 0.5\n",
    "calculate_confusion_matrix_and_cost(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### Automate the Cost Analysis\n",
    "* By executing the above function multiple times with different churn_factor values.\n",
    "* Plot them on a graph\n",
    "* Not down the cut off where you get the BEST results\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cost analysis 10 times\n",
    "cutoffs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.9]\n",
    "\n",
    "# USE this for larger number of rus *BUT* it would take time\n",
    "# cutoffs = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "# Create a datafram to hold the churn factor & cost\n",
    "analysis_df=pd.DataFrame(columns=['churn_factor', 'cost']) \n",
    "i=0\n",
    "for c in cutoffs:\n",
    "    print(\"-\",end=\" \")\n",
    "    analysis_df.loc[i, 'churn_factor']=c\n",
    "    cost, cm = calculate_confusion_matrix_and_cost(c)\n",
    "    analysis_df.loc[i, 'cost']=cost\n",
    "    i += 1\n",
    "print(\"!\")\n",
    "\n",
    "# Lets plot a chart\n",
    "%matplotlib inline\n",
    "plt.plot(analysis_df['churn_factor'],analysis_df['cost'], marker='o')\n",
    "plt.xlabel('Churn Factor')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Churn Factor Vs. Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### 6. Evaluate standard metrics used for **Binary Classification**\n",
    "\n",
    "* <code style=\"background:yellow;color:black\">Accuracy </code> =  (TP + TN) / (TN + FP + TP + FN)\n",
    "   <br> An accuracy of 1 *may* indicate that the model is overfitting.\n",
    "    \n",
    "* <code style=\"background:yellow;color:black\">Precision</code> = TP / (TP + FP) \n",
    "    <br> Percentage of correctly predictive positive values <br>\n",
    "\n",
    "* <code style=\"background:yellow;color:black\">Sensitivity or Recall</code> = TP / (TP + FN)\n",
    "    * Also referred to as *Recall*\n",
    "    * Also referred to as *True Positive Recall*\n",
    "    * High sensitivity desired when cost of FN is HIGH e.g., predicting \"Not Fraud\" for \"Actual Fraud\"\n",
    "      <br> To increase sensitivity - reduce FN which *may* increase FP\n",
    "\n",
    "    \n",
    "* <code style=\"background:yellow;color:black\">Specificity</code> = TN / (TN + FP)\n",
    "    * Also referred to as *True Negative Rate*\n",
    "    * High specificity desired when cost of FP is HIGH e.g., marking an email as spam\n",
    "    * *False Positive Rate* = (1 - Specificity) <br>\n",
    "\n",
    "    \n",
    "* <code style=\"background:yellow;color:black\">F1 Score</code> = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    <br> Harmonic Average of *Specificity* and *Sensitivity*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to the BEST churn factor value\n",
    "cutoff = 0.4\n",
    "\n",
    "# Confusion matrix\n",
    "cost, c_matrix = calculate_confusion_matrix_and_cost(cutoff)\n",
    "\n",
    "# Get the confusion matrix elements\n",
    "TP = c_matrix.loc['predicted_true', 'actual_true']\n",
    "FP = c_matrix.loc['predicted_true', 'actual_false']\n",
    "FN = c_matrix.loc['predicted_false', 'actual_true']\n",
    "TN = c_matrix.loc['predicted_false', 'actual_false']\n",
    "\n",
    "metrics_df=pd.DataFrame(index=[\"accuracy\",\"precision\",\"recall\",\"specificity\",\"f1 score\" ],columns=['metric']) \n",
    "\n",
    "# Accuracy\n",
    "metrics_df.loc['accuracy', 'metric'] = (TN + TP)/(TP + TN + FP + FN)\n",
    "\n",
    "# Precision\n",
    "precision = TP / (TP + FP)\n",
    "metrics_df.loc['precision', 'metric']  = precision\n",
    "\n",
    "# Recall\n",
    "recall = TP / (TP + FN)\n",
    "metrics_df.loc['recall', 'metric']  = recall\n",
    "\n",
    "\n",
    "# Specificity \n",
    "metrics_df.loc['specificity', 'metric'] = TN / (TN + FP)\n",
    "\n",
    "# F1 Score\n",
    "\n",
    "metrics_df.loc['f1 score', 'metric'] = 2 * (precision * recall) / ( precision)\n",
    "\n",
    "# Display the metrics\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
