{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### Try out the XGBoost algo with different hyperparameters\n",
    "\n",
    "#### 0. Setup the imports and bucket reference\n",
    "#### 1. Get the image URI for the XGBoost algo\n",
    "#### 2. Setup the Input and Output data locations\n",
    "#### 3. Execute a training job\n",
    "#### 4. Manually tuning the model\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Couldn't call 'get_role' to get Role ARN from role name my-sagemaker-studio-role to get Role path.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# CHANGE the bucket name\n",
    "bucket = \"awsrajeev\"\n",
    "\n",
    "# This will be a folder created under your bucket \n",
    "prefix = \"sagemaker/churn-analysis\"\n",
    "\n",
    "\n",
    "# Get the role - ignore the warning\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### 1. Get the SageMaker container implementation for the algo\n",
    "* SageMaker implements the ML algos in Docker containers (images)\n",
    "* The images are maintained in *Elastic Container Registry (ECR)*\n",
    "* The api *get_image_uri* gets a reference to the specified algo container image\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# Ignore the warning\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost',repo_version='0.90-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### 2. Setup the Input and Output data locations\n",
    "    \n",
    "##### Input\n",
    "* Ensure that bucket are prefix are correct\n",
    "* Notice the content_type= csv; the data may be specified in other formats; depends on the algo\n",
    "* Notice that we have specified the folder NOT the file as the data may spread across multiple files\n",
    "\n",
    "##### Output\n",
    "* Training job writes the model artefacts to S3\n",
    "* A new S3 folder is created with the name of the training job\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will be passed as the parameters to the algo\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_test = sagemaker.s3_input(s3_data='s3://{}/{}/test/'.format(bucket, prefix), content_type='csv')\n",
    "\n",
    "# we will write all outputs under the folder output\n",
    "s3_output_model='s3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### 3. Execute a training job\n",
    "    \n",
    "* Provide a job name - if not provided the default name is created *[Algo Name]-[Timestamp]*\n",
    "* The job name MUST be unique every time you execute a training job \n",
    "    <code style=\"background:yellow;color:black\">otherwise training job will fail will fail to launch!!!</code>\n",
    "* You may checkout the training jobs using:\n",
    "    * Console\n",
    "    * API\n",
    "    \n",
    "   \n",
    "##### Estimator parameters\n",
    "* container - image for the algo\n",
    "* role - determines the permissions that the training job will have; fix this for permission errors\n",
    "* train_instance - depends on the job; smaller machine longer runtime or even a failure\n",
    "    \n",
    "##### Starting the training job\n",
    "\n",
    "* The algo.<code style=\"background:yellow;color:black\">fit()</code> method starts the execution of the job\n",
    "* MUST provide the data locations; \n",
    "    * channel refers to the type of data; train channel = training data, validation channel = validation data\n",
    "    * channels depend on the algo\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-14 10:42:07 Starting - Starting the training job...\n",
      "2020-05-14 10:42:09 Starting - Launching requested ML instances......\n",
      "2020-05-14 10:43:18 Starting - Preparing the instances for training......\n",
      "2020-05-14 10:44:31 Downloading - Downloading input data\n",
      "2020-05-14 10:44:31 Training - Downloading the training image...\n",
      "2020-05-14 10:45:08 Uploading - Uploading generated training model\n",
      "2020-05-14 10:45:08 Completed - Training job completed\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[10:44:58] 2333x69 matrix with 160977 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[10:44:58] 666x69 matrix with 45954 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 2333 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 666 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.077154#011validation-error:0.099099\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.050579#011validation-error:0.081081\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.048864#011validation-error:0.075075\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.046721#011validation-error:0.072072\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.048007#011validation-error:0.073574\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.046721#011validation-error:0.070571\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.045435#011validation-error:0.073574\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.043721#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.045006#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.042435#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.040291#011validation-error:0.064565\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.039006#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.038577#011validation-error:0.064565\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.03772#011validation-error:0.064565\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.03772#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.039434#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.038577#011validation-error:0.063063\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.03772#011validation-error:0.064565\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.039434#011validation-error:0.063063\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.039863#011validation-error:0.06006\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.039434#011validation-error:0.063063\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.038577#011validation-error:0.063063\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.038148#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.036862#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.036005#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.034291#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.033862#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.033862#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.033862#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.033862#011validation-error:0.072072\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.033862#011validation-error:0.070571\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.034291#011validation-error:0.072072\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.034719#011validation-error:0.070571\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.034719#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.033005#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.033862#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.033862#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.033005#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.033433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.033005#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.031719#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.031719#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.030862#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.03129#011validation-error:0.069069\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.030862#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.03129#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.030862#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.030862#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.030433#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.030004#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.030004#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.030004#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.030004#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.030004#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.030004#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.030433#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.029147#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.029147#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.029147#011validation-error:0.067568\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.028718#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.029147#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.029147#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.029147#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.029147#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.029147#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.029147#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.029576#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.030004#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.029576#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.02829#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.02829#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.02829#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.027861#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.027861#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.027861#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.027861#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.02829#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.02829#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.027861#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.027861#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.02829#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.027861#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.027861#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.027861#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.02829#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.02829#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.02829#011validation-error:0.066066\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.02829#011validation-error:0.066066\u001b[0m\n",
      "Training seconds: 59\n",
      "Billable seconds: 59\n"
     ]
    }
   ],
   "source": [
    "# The training job name - change the number to 102, 103 ... timestamp appended to it automaticaly\n",
    "job_name = \"xgboost-my-training-job-101\"\n",
    "\n",
    "# Session is used for connecting to the Sagemaker service\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "\n",
    "# Instance of the algo\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path=s3_output_model,\n",
    "                                    sagemaker_session=sess,\n",
    "                                    base_job_name=job_name)\n",
    "\n",
    "# Setup the hyperparameters; objective is to predict Yes | No so 'binary:logistic'\n",
    "objective = 'binary:logistic'\n",
    "\n",
    "# Tuning involves adjusting the values of Hyperparameters\n",
    "xgb.set_hyperparameters(objective=objective,\n",
    "                        max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        num_round=100)\n",
    "\n",
    "# Start the training job - provide the training and validation data locations\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 1. Checkout the logs generated\n",
    "\n",
    "* The idea is to MINIMIZE the train-error |  validation-error\n",
    "* Algorithm is iterating through the runs to get to that minimal for the error\n",
    "* Checkout the error values in the last 10 to 15 logs - is it changing? If NO then that means we are getting there :)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 2. Checkout the status of the training job in the console\n",
    "\n",
    "* You may also get the training job details using the AWS CLI | API\n",
    "* All of the training jobs are listed; you may use the details to compare model performance\n",
    "* In the details you will find the Hyperparameters used for the training job\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 3. Checkout the Model artefacts generated in the S3 bucket\n",
    "\n",
    "* Use the console \n",
    "* You may download the artefacts and look into the contents if you are interested\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### 4. Manually tuning the model\n",
    "* Based on the run you may decide to adjust the Hyperparameters and run the job again\n",
    "* Checkout the Hyperparameter definitions below; these apply to XGBoost. Each algo has its own set of Hyperparameters\n",
    "* The process of tuning is iterative; so you may end up running the job 10-20-30- ... times !!!!\n",
    "    \n",
    "**Go ahead run the job a couple of times .<code style=\"background:yellow;color:black\">with different Hyperparameter values</code>. For each run the model will be genaretd in a S3 under a different folder.**\n",
    "\n",
    "    \n",
    "### Setup the hyperparameters\n",
    "    \n",
    "<dl>\n",
    "    <dt>max_depth</dt>\n",
    "    <dd>controls how deep each tree within the algorithm can be built. Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting. There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.</dd>\n",
    "    <dt>subsample</dt>\n",
    "    <dd>controls sampling of the training data. This technique can help reduce overfitting, but setting it too low can also starve the model of data.</dd>\n",
    "    <dt>num_round</dt> \n",
    "    <dd>controls the number of boosting rounds. This is essentially the subsequent models that are trained using the residuals of previous iterations. Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.</dd>\n",
    "    <dt>eta</dt> \n",
    "    <dd>controls how aggressive each round of boosting is. Larger values lead to more conservative boosting.</dd>\n",
    "    <dt>gamma</dt>\n",
    "    <dd>controls how aggressively trees are grown. Larger values lead to more conservative models.</dd>\n",
    "    \n",
    "</dl>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
