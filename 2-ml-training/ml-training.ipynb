{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### Try out the XGBoost algo with different hyperparameters\n",
    "\n",
    "#### 0. Setup the imports and bucket reference \n",
    "#### 1. Get the image URI for the XGBoost algo\n",
    "#### 2. Setup the Input and Output data locations\n",
    "#### 3. Execute a training job\n",
    "#### 4. Manually tune the model\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:yellow;color:black\">You MUST change S3 Bucket name in the next cell</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import random\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# CHANGE the bucket name\n",
    "bucket = \"rajeev-65120\"\n",
    "\n",
    "# This will be a folder from where data is read & model is written\n",
    "prefix = \"sagemaker/churn-analysis\"\n",
    "\n",
    "\n",
    "# Get the role - ignore the warning\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### 1. Get the SageMaker container implementation for the algo\n",
    "* SageMaker implements the ML algos in Docker containers (images)\n",
    "* The images are maintained in *Elastic Container Registry (ECR)*\n",
    "* The api *get_image_uri* gets a reference to the specified algo container image\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# Latest version may be different across regions\n",
    "repo_version='0.90-2'  # Northern VA\n",
    "# repo_version = '1.0-1'   # OH\n",
    "\n",
    "# Ignore the warning\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost',repo_version=repo_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### 2. Setup the Input and Output data locations\n",
    "    \n",
    "##### Input\n",
    "* Ensure that bucket are prefix are correct\n",
    "* Notice the content_type= csv; the data may be specified in other formats; depends on the algo\n",
    "* Notice that we have specified the folder NOT the file as the data may spread across multiple files\n",
    "\n",
    "##### Output\n",
    "* Training job writes the model artefacts to S3\n",
    "* A new S3 folder is created with the name of the training job\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will be passed as the parameters to the algo\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_test = sagemaker.s3_input(s3_data='s3://{}/{}/test/'.format(bucket, prefix), content_type='csv')\n",
    "\n",
    "# we will write all outputs under the folder output\n",
    "s3_output_model='s3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### 3. Execute a training job\n",
    "    \n",
    "* Provide a job name - if not provided the default name is created *[Algo Name]-[Timestamp]*\n",
    "* The job name MUST be unique every time you execute a training job \n",
    "    <code style=\"background:yellow;color:black\">otherwise training job will fail will fail to launch!!!</code>\n",
    "* You may checkout the training jobs using:\n",
    "    * Console\n",
    "    * API\n",
    "    \n",
    "   \n",
    "##### Estimator parameters\n",
    "* container - image for the algo\n",
    "* role - determines the permissions that the training job will have; fix this for permission errors\n",
    "* train_instance - depends on the job; smaller machine longer runtime or even a failure\n",
    "    \n",
    "##### Starting the training job\n",
    "\n",
    "* The algo.<code style=\"background:yellow;color:black\">fit()</code> method starts the execution of the job\n",
    "* MUST provide the data locations; \n",
    "    * channel refers to the type of data; train channel = training data, validation channel = validation data\n",
    "    * channels depend on the algo\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'form'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-344763037224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mnum_round_rand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# You may fix to a number e.g., num_round_rand=100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_depth_rand={} ,min_child_weight_rand={} ,num_round_rand={}  \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth_rand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight_rand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_round_rand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m xgb.set_hyperparameters(objective=objective,\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'form'"
     ]
    }
   ],
   "source": [
    "# The training job name - change the number to 102, 103 ... timestamp appended to it automaticaly\n",
    "job_name = \"xgboost-my-training-job-101\"\n",
    "\n",
    "# Session is used for connecting to the Sagemaker service\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "\n",
    "# Instance of the algo\n",
    "# You may even use spot instances; which will translate into saving of upto 80%\n",
    "#        train_use_spot_instances=True\n",
    "# You can control the run away jobs\n",
    "#        train_max_run=300\n",
    "#        train_max_wait=600\n",
    "# Sagemaker Debugger can help with managing the working of Algo during the training\n",
    "#        debugger_hook_config=DebuggerHookConfig(...)\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.2xlarge',\n",
    "                                    output_path=s3_output_model,\n",
    "                                    sagemaker_session=sess,\n",
    "                                    base_job_name=job_name)\n",
    "\n",
    "# Setup the hyperparameters; objective is to predict Yes | No so 'binary:logistic'\n",
    "objective = 'binary:logistic'\n",
    "\n",
    "# Tuning involves adjusting the values of Hyperparameters\n",
    "#     The parameter eval_metric may be set for metric used for job evaluatio.\n",
    "#     By default it is the validation : error\n",
    "#     You may try eval_metric='auc' to use 'Area Under Curve'\n",
    "# ADJUST the parameters to tune - DO NOT CHANGE THE 'objective'\n",
    "\n",
    "# First time run through with the random - then try fixing the value for these hyperparameters:\n",
    "max_depth_rand=random.randint(2,10)          # You may fix to a number e.g., max_depth_rand=5\n",
    "min_child_weight_rand=random.randint(0,20)   # You may fix to a number e.g., min_child_weight_rand=3\n",
    "num_round_rand=random.randint(50,150)        # You may fix to a number e.g., num_round_rand=100\n",
    "\n",
    "print(\"max_depth_rand={} ,min_child_weight_rand={} ,num_round_rand={}  \".format(max_depth_rand, min_child_weight_rand,num_round_rand))\n",
    "\n",
    "xgb.set_hyperparameters(objective=objective,\n",
    "                        max_depth=max_depth_rand,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=min_child_weight_rand,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        num_round=num_round_rand)\n",
    "\n",
    "# Start the training job - provide the training and validation data locations\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 1. Checkout the logs generated\n",
    "\n",
    "* The idea is to MINIMIZE the train-error |  validation-error\n",
    "* Algorithm is iterating through the runs to get to that minimal for the error\n",
    "* Checkout the error values in the last 10 to 15 logs - is it changing? If NO then that means we are getting there :)\n",
    "* Training job's cost is calculated based on number of seconds - how much time did your job take?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 2. Checkout the Model artefacts generated in the S3 bucket\n",
    "\n",
    "* Use the console \n",
    "* You may download the artefacts and look into the contents if you are interested (optional)\n",
    "    * Using the console download the file from\n",
    "        * s3://Your-Bucket-Name/Sagemaker/churn-analysis/output/[Training folder]/model.tar.gz\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "### 4. Manually tuning the model\n",
    "* Based on the run you may decide to adjust the Hyperparameters and run the job again\n",
    "* Checkout the Hyperparameter definitions below; these apply to XGBoost. Each algo has its own set of Hyperparameters\n",
    "* The process of tuning is iterative; so you may end up running the job 10-20-30- ... times !!!!\n",
    "    \n",
    "**Go ahead run the job a couple of times .<code style=\"background:yellow;color:black\">with different Hyperparameter values</code>. \n",
    "\n",
    "1. Change the Hyperparameters <code style=\"background:yellow;color:black\">Do NOT change the objective!!!</code>\n",
    "2. You may change the Job Name and Run the cells (you may Run All Cells)\n",
    "3. Check the console > Training Jobs to review results and select the best Job; minimal validation errors.\n",
    "\n",
    "    * For each run the model will be genaretd in a S3 under a different folder.**\n",
    "\n",
    "    \n",
    "### Hyperparameters for XGBoost\n",
    "[Read the details](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)\n",
    "    \n",
    "<dl>\n",
    "    <dt>max_depth</dt>\n",
    "    <dd>controls how deep each tree within the algorithm can be built. Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting. There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.</dd>\n",
    "    <dt>subsample</dt>\n",
    "    <dd>controls sampling of the training data. This technique can help reduce overfitting, but setting it too low can also starve the model of data.</dd>\n",
    "    <dt>num_round</dt> \n",
    "    <dd>controls the number of boosting rounds. This is essentially the subsequent models that are trained using the residuals of previous iterations. Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.</dd>\n",
    "    <dt>eta</dt> \n",
    "    <dd>controls how aggressive each round of boosting is. Larger values lead to more conservative boosting.</dd>\n",
    "    <dt>gamma</dt>\n",
    "    <dd>controls how aggressively trees are grown. Larger values lead to more conservative models.</dd>\n",
    "    \n",
    "</dl>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### 2. Checkout the status of the training job in the console or with CLI\n",
    "\n",
    "* \n",
    "* All of the training jobs are listed; you may use the details to compare model performance\n",
    "* In the details you will find the Hyperparameters used for the training job\n",
    "<br>\n",
    "* You may also get the training job details using the AWS CLI | API\n",
    "    * Open the terminal and run the command\n",
    "    *  **aws sagemaker list-training-jobs | grep TrainingJobName**\n",
    "    * **aws sagemaker describe-training-job --training-job-name  [[Copy and Paste full job name from previous command]]**\n",
    "        * Each job has the metrics that determine how good it is\n",
    "        * ```\n",
    "        \"FinalMetricDataList\": [\n",
    "        {\n",
    "            \"MetricName\": \"train:error\",\n",
    "            \"Value\": 0.028289999812841415,\n",
    "            \"Timestamp\": 1589925.549\n",
    "        },\n",
    "        {\n",
    "            \"MetricName\": \"validation:error\",\n",
    "            \"Value\": 0.06606999784708023,\n",
    "            \"Timestamp\": 1589925.549\n",
    "        }\n",
    "    ```\n",
    "        * Compare the **validation:error** for each job and chose the BEST one (lowest error)\n",
    "    * <code style=\"background:yellow;color:black\">Copy the job name for the BEST model to a temporary file</code> - notepad on Windows, Wordpad on Mac - you will need it later :)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws sagemaker list-training-jobs | grep TrainingJobName"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
